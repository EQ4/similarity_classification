{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "seaborn.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_curve(file_name='', intervals=None, labels=None, scores=None, norm=None):\n",
    "    \n",
    "    label_agreement = np.zeros((len(labels), len(labels)), dtype=bool)\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i, len(labels)):\n",
    "            label_agreement[i, j] = (labels[i] == labels[j])\n",
    "            label_agreement[j, i] = label_agreement[i, j]\n",
    "    \n",
    "    time_norm = 1\n",
    "    \n",
    "    durations = np.diff(intervals, axis=1).ravel()\n",
    "    \n",
    "    if norm == 'min':\n",
    "        time_norm = np.minimum.outer(durations, durations)\n",
    "        \n",
    "    elif norm == 'max':\n",
    "        time_norm = np.maximum.outer(durations, durations)\n",
    "        \n",
    "    elif norm == 'hmean':\n",
    "        time_norm = 2./np.add.outer(durations, durations)\n",
    "        time_norm *= np.multiply.outer(durations, durations)\n",
    "    \n",
    "    # TODO: have the label agreement index out nan-valued scores\n",
    "    \n",
    "    scores = scores / time_norm\n",
    "    \n",
    "    label_agreement[np.tril_indices_from(label_agreement, k=0)] = False\n",
    "    \n",
    "    label_agreement[~np.isfinite(scores)] = False\n",
    "    \n",
    "    label_disagreement = ~label_agreement\n",
    "    \n",
    "    label_disagreement[np.tril_indices_from(label_disagreement, k=0)] = False\n",
    "    \n",
    "    label_disagreement[~np.isfinite(scores)] = False\n",
    "    \n",
    "    tp_scores = scores[label_agreement]\n",
    "    fp_scores = scores[label_disagreement]\n",
    "    \n",
    "    num_pos = np.sum(label_agreement)\n",
    "    num_neg = np.sum(label_disagreement)\n",
    "    \n",
    "    y_true = np.concatenate([np.zeros(len(tp_scores)), np.ones(len(fp_scores))])\n",
    "    y_score = np.concatenate([tp_scores, fp_scores])\n",
    "    \n",
    "    fpr, tpr, thr = sklearn.metrics.roc_curve(y_true, y_score)\n",
    "    \n",
    "    tp = num_pos * tpr\n",
    "    fp = num_neg * fpr\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tpr\n",
    "    \n",
    "    fmeasure = np.asarray([mir_eval.util.f_measure(p, r) for p, r in zip(precision, recall)])\n",
    "    \n",
    "    k = np.argmax(fmeasure)\n",
    "    thr_opt = thr[k]\n",
    "    \n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--', alpha=0.5)\n",
    "    \n",
    "#     plt.plot(fpr, tpr)\n",
    "#     plt.plot(fpr[k], tpr[k], color='r', marker='*', markersize=10, alpha=0.5)\n",
    "    \n",
    "#     plt.xlim([0, 1])\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.xlabel('FPR')\n",
    "#     plt.ylabel('TPR')\n",
    "#     plt.title(file_name)\n",
    "    \n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.plot(precision, recall)\n",
    "#     plt.xlabel('Precision')\n",
    "#     plt.ylabel('Recall')\n",
    "#     plt.title('norm={}'.format(norm))\n",
    "    \n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.plot(thr, fmeasure)\n",
    "#     k = np.argmax(fmeasure)\n",
    "#     plt.plot(thr[k], fmeasure[k], marker='*', markersize=10, alpha=0.5, color='r')\n",
    "    \n",
    "#     plt.xlabel(r'$\\theta$')\n",
    "#     plt.ylabel('$F_1$')\n",
    "#     plt.title(r'({:.3f}, {:.3f})'.format(thr[k], fmeasure[k]))\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "    return thr[k], fmeasure[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vars().update(pickle.load(open('../data/scores_datasetESALAMI_levelElarge_scale_distEL2.pk', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30.873171917682594, 0.50237326751471412)\n",
      "(1.3407262893098728, 0.3621008181578253)\n",
      "(0.63059757758166768, 0.65116279069767447)\n",
      "(1.0164182735129479, 0.3621008181578253)\n"
     ]
    }
   ],
   "source": [
    "# k = 425\n",
    "# for norm in [None, 'min', 'max', 'hmean']:\n",
    "    \n",
    "#     print plot_curve(norm=norm, **file_scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
